{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "253bbbd1-6d9b-4216-8abd-bd5849982232",
   "metadata": {},
   "source": [
    "# Expanding the existing intents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ed3682-2049-44de-84bf-68d14408dccd",
   "metadata": {},
   "source": [
    "I order to find the right desease we need to get the right synonyms from the user. Hence in our case, every synonym is one intent.\n",
    "Here you have to keep in mind, that every symptom can have similiar descriptions to other symptoms. Hence it is better to only use as few keywords as needed in order to get as few similarities between the different intents/symptoms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5f7b66-5b4b-48d1-af73-814bb9315502",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b8fb10-11b7-4518-ae37-f06f75467d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import standard libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import support libs\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "#import nlp libs\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_sci_lg')\n",
    "\n",
    "#open hand-written intents\n",
    "with open('intents/intents.json', 'r+') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96e4c4b3-1991-4ad2-9246-56679574812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new intents as dictionary\n",
    "data = dict()\n",
    "data['intents'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f275606-af8b-4f57-8843-3f1f4b5013f6",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76f11e-f8f3-4de8-a80a-c3063ad801b7",
   "metadata": {},
   "source": [
    "The following script extends the existing patterns of every intent with synonyms of simple tags of patterns. \\n\n",
    "Simple means, that many tags consist of one or two words. These tags are considered as simple, as synonyms can be composed easily. \\n\n",
    "Some tags like \"prominent_veins_on_calf\" are more complex and finding synonyms of them are harder to find and not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6eb4f8b-5adf-43ba-a765-f322d29edbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(intents['intents'])):\n",
    "        \n",
    "    intent = intents['intents'][i]\n",
    "    tag = intent['tag']\n",
    "    pattern = intent['patterns']\n",
    "\n",
    "    \n",
    "    #single words like headache\n",
    "    \n",
    "    #get synonyms for the tag\n",
    "    synonyms = wordnet.synsets(tag)\n",
    "    new_words = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "    new_words = [word.replace('_', ' ') for word in new_words] #this is a list with every synonym found by the wordnet from nltk\n",
    "    \n",
    "    docs = []\n",
    "    for item in new_words:\n",
    "        \n",
    "        #filter out synonyms that have no scientific background\n",
    "        #note that this step was due to weird synonyms like \"make out\" as a synonym for \"neck\" (the body part)\n",
    "        #we only could use a scientific dataset from sci-spacy but would have favored a medical dataset here. (More on that at the end)\n",
    "        doc = list(nlp(item).ents)\n",
    "\n",
    "        X = []\n",
    "        for x in doc:\n",
    "            x = str(x)\n",
    "            X.append(x)\n",
    "\n",
    "        X = ' '.join(X)\n",
    "        docs.append(X)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pattern.extend(docs)\n",
    "    \n",
    "    #two words like neck_pain\n",
    "    #same process es above but for each of the two words we find synonyms each, then filter them and then combine them to one pattern.\n",
    "    sentence = tag.split('_')\n",
    "    if len(sentence) == 2:\n",
    "        liste=[]\n",
    "        for part in sentence:\n",
    "            synonyms = wordnet.synsets(part)\n",
    "            part = set(chain.from_iterable([word.lemma_names() for word in synonyms]))\n",
    "            part = [word.replace('_', ' ') for word in part]\n",
    "\n",
    "            docs= []\n",
    "            for item in part:\n",
    "                doc = list(nlp(item).ents)\n",
    "\n",
    "                X = []\n",
    "                for x in doc:\n",
    "                    x = str(x)\n",
    "\n",
    "                    X.append(x)\n",
    "                    \n",
    "                X = ' '.join(X)\n",
    "                docs.append(X)\n",
    "\n",
    "            liste.append(docs)\n",
    "\n",
    "        liste2=[]\n",
    "        for part in liste:\n",
    "            part = [x for x in part if x]\n",
    "            liste2.append(part)\n",
    "\n",
    "        liste2 = list(itertools.product(liste2[0], liste2[1]))\n",
    "        \n",
    "        for sentence in liste2:\n",
    "            string = ' '.join(sentence) \n",
    "\n",
    "            pattern.append(string)\n",
    "            \n",
    "    ###########################################################        \n",
    "    \n",
    "    #add pattern and the belonging tag/intent/synonym to the intents\n",
    "    \n",
    "    pattern = list(set(pattern))\n",
    "    \n",
    "    dictionary = dict()\n",
    "    dictionary['tag']=tag\n",
    "    dictionary['patterns']=pattern\n",
    "    \n",
    "    data['intents'].append(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6177ce44-38fa-4858-a226-3017684575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save intents as new intents\n",
    "json_object = json.dumps(data, indent = 4)\n",
    "\n",
    "with open(\"intents_new2.json\", \"w\") as outfile:\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe486bb-8716-4861-8f98-0a485b31d9ee",
   "metadata": {},
   "source": [
    "Note that this is our first idea of trying to expand the patterns of our intents.\n",
    "Other ways would involve excessive text and data mining to find descriptions. Normal people describe their symptoms with simple words but long explanations which means to find such descriptions. This could, however, lead to more problems like described in the top of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238f323-ec46-4c49-a8da-406ff45d2f20",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9114650-7129-4198-a3cb-edf82752527e",
   "metadata": {},
   "source": [
    "The following code snippet is the beginning of the usage of the UMLS from the NIH (a huge medical database).\n",
    "This was meant to become the alternative of the scientific dataset from above. There would be other uses for it though, as the database could become the foundation of all the intents but lead to a huge amount of data management and preprocessing.\n",
    "In order to use this dataset of medical terminologies you need to register on the site and apply for access, therefore this approach was not covered up to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef6ae14-e8e0-4174-87d4-d31aa67ea1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pymedtermino as med\n",
    "#med.LANGUAGE = 'en'\n",
    "#med.REMOVE_SUPRESSED_CONCEPTS = True\n",
    "#from pymedtermino import *\n",
    "#from pymedtermino.snomedct import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
